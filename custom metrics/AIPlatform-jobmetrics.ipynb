{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import google.auth\n",
    "import arrow\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pandas.io.json import json_normalize\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "client = bigquery.Client(location=\"US\", project=\"sb-bigdata-4985-da852265\")\n",
    "\n",
    "def get_api(url):\n",
    "    import google.auth\n",
    "    credentials, project = google.auth.default()\n",
    "    from google.auth.transport.requests import AuthorizedSession\n",
    "    authed_session = AuthorizedSession(credentials)\n",
    "    response = authed_session.get(url)\n",
    "    return response\n",
    "\n",
    "def post_api(url, data):\n",
    "    import google.auth\n",
    "    credentials, project = google.auth.default()\n",
    "    from google.auth.transport.requests import AuthorizedSession\n",
    "    authed_session = AuthorizedSession(credentials)\n",
    "    response = authed_session.post(url, data)\n",
    "    return response\n",
    "\n",
    "def get_createdby(createTime, jobId):\n",
    "    createdBy=\"NA\"\n",
    "    import arrow\n",
    "    import json\n",
    "    createTime = arrow.get(createTime)\n",
    "    logstart=createTime + datetime.timedelta(minutes = -2)\n",
    "    logend=createTime + datetime.timedelta(minutes = 2)\n",
    "    body = {\"resourceNames\": [\"projects/sb-bigdata-4985-da852265\"], \"filter\":'resource.type=\"audited_resource\" \\\n",
    "    resource.labels.service=\"ml.googleapis.com\" \\\n",
    "    resource.labels.method=\"google.cloud.ml.v1.JobService.CreateJob\" \\\n",
    "    timestamp>=\"' + str(logstart) + '\" \\\n",
    "    timestamp<=\"' + str(logend) + '\"'\n",
    "    }\n",
    "    y = json.dumps(body)\n",
    "    response=post_api('https://logging.googleapis.com/v2/entries:list', y)\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    data_df = pd.read_json(response.text)\n",
    "    if 'entries' in data_df:\n",
    "        for job in data_df['entries']:\n",
    "            if job[\"protoPayload\"][\"request\"][\"job\"][\"jobId\"] == jobId:\n",
    "                createdBy=job[\"protoPayload\"][\"authenticationInfo\"][\"principalEmail\"]\n",
    "                break\n",
    "    else:\n",
    "        if response.status_code == 429:\n",
    "            import time\n",
    "            time.sleep(1)\n",
    "            createdBy=get_createdby(createTime, jobId)\n",
    "            \n",
    "    return createdBy\n",
    "\n",
    "def get_labels_df(createTime, jobId):\n",
    "    labels=\"NA\"\n",
    "    import arrow\n",
    "    import pandas as pd\n",
    "    createTime = arrow.get(createTime)\n",
    "    logstart=createTime + datetime.timedelta(minutes = -2)\n",
    "    logend=createTime + datetime.timedelta(minutes = 2)\n",
    "    body = {\"resourceNames\": [\"projects/sb-bigdata-4985-da852265\"], \"filter\":'resource.type=\"audited_resource\" \\\n",
    "    resource.labels.service=\"ml.googleapis.com\" \\\n",
    "    resource.labels.method=\"google.cloud.ml.v1.JobService.CreateJob\" \\\n",
    "    timestamp>=\"' + str(logstart) + '\" \\\n",
    "    timestamp<=\"' + str(logend) + '\"'\n",
    "    }\n",
    "    y = json.dumps(body)\n",
    "    response=post_api('https://logging.googleapis.com/v2/entries:list', y)\n",
    "    data_df = pd.read_json(response.text)\n",
    "    if 'entries' in data_df:\n",
    "        for job in data_df['entries']:\n",
    "            if job[\"protoPayload\"][\"request\"][\"job\"][\"jobId\"] == jobId and \"labels\" in job[\"protoPayload\"][\"request\"][\"job\"]:\n",
    "                labels=job[\"protoPayload\"][\"request\"][\"job\"][\"labels\"]\n",
    "                break\n",
    "    else:\n",
    "        if response.status_code == 429:\n",
    "            import time\n",
    "            time.sleep(1)\n",
    "            labels=get_labels_df(createTime, jobId)\n",
    "            \n",
    "    return labels\n",
    "\n",
    "def get_trainingEndTime(createTime, job_Id):\n",
    "    trainingEndTime=\"NA\"\n",
    "    page_token = ''\n",
    "    createTime = arrow.get(createTime)\n",
    "  \n",
    "    filter_str = 'resource.labels.job_id=\"' + str(job_Id) + '\" \\\n",
    "    jsonPayload.message=\"Module completed; cleaning up.\" OR \"migration complete.\" OR \"Execution halted\"'\n",
    "    body = {\"resourceNames\": [\"projects/sb-bigdata-4985-da852265\"],\"pageToken\" :str(page_token), \"filter\":filter_str }\n",
    "    y = json.dumps(body)\n",
    "    response=post_api('https://logging.googleapis.com/v2/entries:list', y) \n",
    "    if response.status_code == 200:     \n",
    "        while True:\n",
    "            data_df = pd.read_json(response.text, typ='series')\n",
    "            if 'entries' in data_df and data_df['entries'][0]['resource']['labels']['job_id'] == job_Id:\n",
    "                trainingEndTime = data_df['entries'][0]['timestamp']\n",
    "                break    \n",
    "            else:\n",
    "                page_token = None\n",
    "                page_token = data_df.get('nextPageToken')\n",
    "                if page_token is None:\n",
    "                    break\n",
    "                else:\n",
    "                    body = {\"resourceNames\": [\"projects/sb-bigdata-4985-da852265\"],\"pageToken\" :str(page_token), \"filter\":filter_str }\n",
    "                    y = json.dumps(body)\n",
    "                    response = post_api('https://logging.googleapis.com/v2/entries:list', y)\n",
    "\n",
    "    return trainingEndTime\n",
    "    \n",
    "def get_jobs_df():\n",
    "    job_list=[]\n",
    "    regions=['us-west1', 'us-west2', 'us-central1', 'us-east1', 'us-east4']\n",
    "    query = \"\"\"\n",
    "    SELECT MAX(createtime) as maxCreateTime\n",
    "    FROM `sb-bigdata-4985-da852265.modelmanagement.job_metrics`;\n",
    "    \"\"\"\n",
    "    query_job = client.query(query,   # Location must match that of the dataset(s) referenced in the query.\n",
    "        location=\"US\")  # API request - starts the query\n",
    "    df = query_job.to_dataframe()\n",
    "    response = get_api('https://ml.googleapis.com/v1/projects/sb-bigdata-4985-da852265/jobs')\n",
    "    if response.status_code == 200:\n",
    "        while True:\n",
    "            import json\n",
    "            import pandas as pd\n",
    "            data_df = pd.read_json(response.text)\n",
    "            for job in data_df['jobs']:\n",
    "                if job['createTime'] > df['maxCreateTime'][0]:\n",
    "                    job[\"trainingEndTime\"]=get_trainingEndTime(job[\"createTime\"], job[\"jobId\"])\n",
    "                    job[\"createdby\"]=get_createdby(job[\"createTime\"], job[\"jobId\"])\n",
    "                    job[\"labels\"]=[get_labels_df(job[\"createTime\"], job[\"jobId\"])]\n",
    "                    if job['state'] != \"CANCELLED\":\n",
    "                        if any(x in job['trainingInput']['region'] for x in regions):\n",
    "                            job[\"trainingCost\"]=0.49*(job['trainingOutput']['consumedMLUnits'])\n",
    "                        else:\n",
    "                            job[\"trainingCost\"]=0.54*(job['trainingOutput']['consumedMLUnits'])\n",
    "                    else:\n",
    "                        job[\"trainingCost\"]=\"NA\"\n",
    "                    job_list.append(job)\n",
    "                    print(job[\"jobId\"])\n",
    "                else:\n",
    "                    None\n",
    "            page_token = None\n",
    "            page_token = data_df.get('nextPageToken')\n",
    "            if page_token is None:\n",
    "                break\n",
    "            else:\n",
    "                response = get_api('https://ml.googleapis.com/v1/projects/sb-bigdata-4985-da852265/jobs' + '?pageToken=' + str(page_token[0]))\n",
    "    \n",
    "    \n",
    "    from pandas.io.json import json_normalize\n",
    "    job_df=json_normalize(job_list)\n",
    "    \n",
    "    fields = pd.DataFrame(columns=['createTime', 'endTime', 'jobId', 'startTime', 'state', 'trainingInput.args', 'trainingInput.jobDir'\n",
    "                                  , 'trainingInput.masterConfig.imageUri', 'createdby', 'trainingInput.masterType', 'trainingInput.packageUris'\n",
    "                                  , 'trainingInput.pythonModule', 'trainingInput.pythonVersion', 'trainingInput.region', 'trainingInput.runtimeVersion'\n",
    "                                  , 'trainingInput.scaleTier', 'trainingOutput.consumedMLUnits', 'labels', 'trainingInput.masterConfig.acceleratorConfig.count'\n",
    "                                  , 'trainingInput.masterConfig.acceleratorConfig.type', 'trainingCost', 'trainingEndTime'])\n",
    "    for col in fields.columns:\n",
    "        if col not in job_df.columns:\n",
    "            job_df[col] = np.nan\n",
    "        else:\n",
    "            None\n",
    "    \n",
    "    job_df_new = job_df[['createTime', 'endTime', 'jobId', 'startTime', 'state', 'trainingInput.args', 'trainingInput.jobDir'\n",
    "                                  , 'trainingInput.masterConfig.imageUri', 'createdby', 'trainingInput.masterType', 'trainingInput.packageUris'\n",
    "                                  , 'trainingInput.pythonModule', 'trainingInput.pythonVersion', 'trainingInput.region', 'trainingInput.runtimeVersion'\n",
    "                                  , 'trainingInput.scaleTier', 'trainingOutput.consumedMLUnits', 'labels', 'trainingInput.masterConfig.acceleratorConfig.count'\n",
    "                                  , 'trainingInput.masterConfig.acceleratorConfig.type', 'trainingCost', 'trainingEndTime']].copy()\n",
    "    \n",
    "    job_metrics_df = job_df_new.rename(columns={\"createTime\": \"createtime\", \"endTime\": \"endtime\", \"jobId\":\"jobid\", \"startTime\":\"starttime\", \"state\":\"state\"\n",
    "                   , \"trainingInput.args\":\"args\", \"trainingInput.jobDir\":\"jobdir\", \"trainingInput.masterConfig.imageUri\":\"imageuri\", \"createdby\":\"createdby\"\n",
    "                   , \"trainingInput.masterType\":\"mastertype\", \"trainingInput.packageUris\":\"packageuris\", \"trainingInput.pythonModule\":\"pythonmodule\"\n",
    "                   , \"trainingInput.pythonVersion\":\"pythonversion\", \"trainingInput.region\":\"region\", \"trainingInput.runtimeVersion\":\"runtimeversion\"\n",
    "                   , \"trainingInput.scaleTier\":\"scaletier\", \"trainingOutput.consumedMLUnits\":\"consumedmlunits\", \"labels\":\"label\"\n",
    "                   , \"trainingInput.masterConfig.acceleratorConfig.count\":\"acceleratorconfigcount\"\n",
    "                   , \"trainingInput.masterConfig.acceleratorConfig.type\":\"acceleratorconfigtype\", \"trainingCost\":\"trainingcost\"\n",
    "                   , \"trainingEndTime\":\"trainingendtime\"})\n",
    "\n",
    "    return job_metrics_df\n",
    "\n",
    "def write_to_bqtable(bq_tablename, bq_schemaname, datadf):\n",
    "    import google.auth\n",
    "    from google.cloud import bigquery\n",
    "    credentials, project = google.auth.default()\n",
    "    client = bigquery.Client(project)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.schema = bq_schemaname\n",
    "    \n",
    "    job = client.load_table_from_dataframe(datadf, bq_tablename, job_config=job_config)\n",
    "    # Wait for the load job to complete.\n",
    "    job.result()\n",
    "\n",
    "def get_data(request):\n",
    "    jobs = []\n",
    "    jobs = get_jobs_df()\n",
    "    print(\"AI platform jobs count : \",str(len(jobs)))\n",
    "    jobs.insert(0, 'date', str(datetime.datetime.utcnow()))\n",
    "    from google.cloud import bigquery\n",
    "    varbqschema=[\n",
    "        bigquery.SchemaField(name=\"date\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"createtime\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"createdby\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"jobid\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"starttime\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"endtime\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"trainingendtime\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"state\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"args\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"jobdir\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"imageuri\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"mastertype\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"packageuris\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"pythonmodule\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"pythonversion\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"region\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"runtimeversion\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"scaletier\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"consumedmlunits\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"label\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"acceleratorconfigcount\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"acceleratorconfigtype\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"trainingcost\", field_type=\"STRING\")\n",
    "    ]\n",
    "    write_to_bqtable('modelmanagement.job_metrics', varbqschema, jobs.astype(str))\n",
    "    return \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-27 10:39:56.786374\n",
      "TPU_fashion_MNIST_20200124_144703\n",
      "TPU_fashion_MNIST_20200123_124443\n",
      "TPU_fashion_MNIST_20200123_121525\n",
      "sentiment__TPU_resolver_strategy_114_20200122_182453\n",
      "sentiment_py3_basic_TPU_20200122_181656\n",
      "sentiment_py3_basic_TPU_20200122_171253\n",
      "sentiment_py3_basic_TPU_20200122_170634\n",
      "sentiment_py3_basic_TPU_20200122_170131\n",
      "sentiment_py3_basic_TPU_20200122_165204\n",
      "sentiment_py3_basic_TPU_20200122_164413\n",
      "sentiment_py3_basic_TPU_20200122_162936\n",
      "sentiment_py3_basic_TPU_20200122_162453\n",
      "sentiment_py3_basic_TPU_20200122_161452\n",
      "sentiment_py3_basic_TPU_20200122_160857\n",
      "sentiment_py3_basic_TPU_20200122_160433\n",
      "AI platform jobs count :  15\n",
      "2020-01-27 10:43:09.676455\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "df = get_data(None)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
